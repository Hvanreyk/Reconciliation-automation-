```python
#!/usr/bin/env python3
"""
Reconcile (Entity × SF) using patterns-only mapping.

Inputs:
  --sf_patterns   SF_Patterns.xlsx (columns: SF Column, SF Key, SF Display, Path String, Match Mode, Enabled)
  --sf_report     Market Data SF Report.xlsx (must contain 'Entity Name' + boolean SF columns)
  --entity_to_taker  Entity_to_Taker.xlsx (columns: Entity Name, Connection ID)
  --ts            Taker Stream Consolidated1.xlsx (columns: Taker Connection ID, Taker Stream ID, Valuation Stream)
  --sd            Settings Default Consolidated gpt.xlsx (columns: Symbol Group Path, Taker Stream ID, Feed to Taker Streams)
  --output        Output .xlsx

Behavior (matches what produced the last file):
  • Match at SF Key level (patterns are OR’d per key; non-overlapping across keys by design).
  • Match modes:
      - PREFIX: case-insensitive startswith + boundary (next char is '/' or '-' or end)
      - EXACT : case-insensitive full-string match (tolerates one trailing '/')
  • Minimal normalization: collapse multiple '/', convert '\'→'/', strip trailing '/', trim.
  • TS mapping via Taker Connection ID → entities; if Taker Stream ID is opaque, expand via SD join on the same Taker Stream ID.
  • SD-only rows counted when SD.Taker Stream ID equals an entity connection ID.
  • SF columns mapped explicitly using the “SF Column” → “SF Key” map from SF_Patterns (case-insensitive exact header match).
  • Output sheets:
      - Results (Entity Name, SF Display, Status)
      - Key->Display
      - Unclassified Observed
      - Overlapping Patterns (only if present)
      - Unknown SF Columns (only if present)
      - Observed (Entity x Key)
"""

import argparse
import re
from pathlib import Path
import pandas as pd


# ----------------------- Helpers -----------------------

def normalize_path(s: str) -> str:
    """Trim, collapse slashes, convert backslashes, drop single trailing slash."""
    s = str(s or "").strip()
    s = re.sub(r"[\\]+", "/", s)      # backslashes → forward
    s = re.sub(r"/{2,}", "/", s)      # collapse duplicate slashes
    s = s.rstrip("/")                 # drop trailing slash
    return s


def ci_equal(a: str, b: str) -> bool:
    return normalize_path(a).lower() == normalize_path(b).lower()


def ci_startswith_with_boundary(s: str, prefix: str) -> bool:
    """Case-insensitive 'startswith' with a boundary rule to avoid false-positives.

    Match if s startswith prefix, and the next character (if any) is one of:
    '/', '-'  OR there is no next character (exact same length).
    """
    s_norm = normalize_path(s).lower()
    p_norm = normalize_path(prefix).lower()
    if not s_norm.startswith(p_norm):
        return False
    if len(s_norm) == len(p_norm):
        return True
    nxt = s_norm[len(p_norm)]
    return nxt in "/-"


def truthy(x):
    if isinstance(x, str):
        return x.strip().lower() in {"1", "true", "t", "yes", "y"}
    if isinstance(x, (int, float)):
        try:
            return int(float(x)) == 1
        except Exception:
            return False
    if isinstance(x, bool):
        return x
    return False


def load_excel_any_cols(path: Path, wanted):
    """Open first sheet that contains all 'wanted' (case/spacing-insensitive) columns."""
    wb = pd.read_excel(path, sheet_name=None, dtype=object)
    for _, df in wb.items():
        cols = {}
        for col in df.columns:
            k = re.sub(r"[^a-z0-9]+", "", str(col).lower())
            for w in wanted:
                if k == re.sub(r"[^a-z0-9]+", "", w.lower()):
                    cols[w] = col
        if len(cols) == len(wanted):
            out = df[[cols[w] for w in wanted]].copy()
            out.columns = wanted
            return out
    raise KeyError(f"{path.name} must contain columns: {wanted} (in any sheet)")


def is_flag_col(series: pd.Series) -> bool:
    """Detects 0/1/yes/no-style boolean columns."""
    s = pd.Series(series).dropna()
    if s.empty:
        return False
    try:
        nums = pd.to_numeric(s, errors="coerce")
        if nums.notna().all() and set(nums.unique()).issubset({0, 1, 0.0, 1.0}):
            return True
    except Exception:
        pass
    vals = s.astype(str).str.strip().str.lower().unique()
    allowed = {"0", "1", "true", "false", "t", "f", "yes", "no", "y", "n"}
    return all(v in allowed for v in vals)


# ----------------------- Core -----------------------

def load_patterns(sf_patterns_path: Path):
    """Read SF_Patterns.xlsx (any sheet with SF Key + Path String). Returns:
       - rules: dict[key] -> list[(mode, path_string)]
       - key_to_display: dict[key] -> display
       - sf_column_map: dict[lower(header)] -> key
    """
    book = pd.read_excel(sf_patterns_path, sheet_name=None, dtype=object)

    # pick sheet containing both 'SF Key' and 'Path String'
    selected_name = None
    for name, df in book.items():
        cols = [re.sub(r"[^a-z0-9]+", "", str(c).lower()) for c in df.columns]
        if "sfkey" in cols and "pathstring" in cols:
            selected_name = name
            break
    if selected_name is None:
        selected_name = list(book.keys())[0]
    df = book[selected_name].copy()

    # column picks (lenient)
    def pick(df0, alts):
        for c in df0.columns:
            if re.sub(r"[^a-z0-9]+", "", c.lower()) in [re.sub(r"[^a-z0-9]+", "", a.lower()) for a in alts]:
                return c
        return None

    col_sf_column = pick(df, ["SF Column", "sf_column", "sfcol"])
    col_sf_key    = pick(df, ["SF Key", "sf_key", "key"]) or (_ for _ in ()).throw(KeyError("Missing 'SF Key'"))
    col_sf_disp   = pick(df, ["SF Display", "sf_display", "display"])
    col_path      = pick(df, ["Path String", "path", "Path"]) or (_ for _ in ()).throw(KeyError("Missing 'Path String'"))
    col_mode      = pick(df, ["Match Mode", "mode"]) or (_ for _ in ()).throw(KeyError("Missing 'Match Mode'"))
    col_enabled   = pick(df, ["Enabled", "enable", "enabled?"])

    use_cols = [c for c in [col_sf_column, col_sf_key, col_sf_disp, col_path, col_mode, col_enabled] if c]
    df = df[use_cols].copy()

    # standardize column names
    std = []
    for c in df.columns:
        k = re.sub(r"[^a-z0-9]+", "", c.lower())
        std.append({
            "sfcolumn": "SF Column",
            "sfkey": "SF Key",
            "sfdisplay": "SF Display",
            "pathstring": "Path String",
            "matchmode": "Match Mode",
            "enabled": "Enabled",
            "enable": "Enabled",
            "enabled?": "Enabled",
        }.get(k, c))
    df.columns = std

    # cleanup values
    df["SF Key"] = df["SF Key"].astype(str).str.strip()
    if "SF Display" in df.columns:
        df["SF Display"] = df["SF Display"].astype(str).str.strip()
    df["Path String"] = df["Path String"].astype(str).str.strip().apply(normalize_path)
    df["Match Mode"] = df["Match Mode"].astype(str).str.strip().str.upper().replace({"": "PREFIX"})
    if "Enabled" in df.columns:
        df["Enabled"] = df["Enabled"].apply(lambda x: truthy(x) if pd.notna(x) else True)
        df = df[df["Enabled"] == True]

    # build outputs
    key_to_display = {}
    if "SF Display" in df.columns:
        for k, g in df.groupby("SF Key"):
            disp = g["SF Display"].dropna()
            key_to_display[k] = (disp.iloc[0] if len(disp) else k)
    else:
        for k in df["SF Key"].dropna().unique():
            key_to_display[k] = k

    rules = {}
    for _, r in df.iterrows():
        key = r["SF Key"]
        mode = "PREFIX" if r["Match Mode"] not in {"PREFIX", "EXACT"} else r["Match Mode"]
        path_str = r["Path String"]
        rules.setdefault(key, []).append((mode, path_str))

    sf_column_map = {}
    if "SF Column" in df.columns:
        for _, r in df.dropna(subset=["SF Column", "SF Key"]).iterrows():
            sfcol = str(r["SF Column"]).strip().lower()
            sf_column_map[sfcol] = r["SF Key"]

    return rules, key_to_display, sf_column_map


def apply_rules_get_keys(s: str, rules: dict) -> set:
    matched = set()
    for k, rlist in rules.items():
        for mode, p in rlist:
            if mode == "EXACT":
                if ci_equal(s, p) or ci_equal(s, p + "/"):
                    matched.add(k)
                    break
            else:  # PREFIX
                if ci_startswith_with_boundary(s, p):
                    matched.add(k)
                    break
    return matched


def build_observed(entity_to_conns, conn_to_entities, ts_df, sd_df, rules):
    """Return:
       - obs_expanded: DataFrame(Entity Name, SF Key)
       - unclassified: list of strings that matched no rule
       - overlaps_df : DataFrame of observed strings that matched >1 key (for diagnostics)
    """
    obs_rows = []

    # TS: map via Taker Connection ID → entities
    for _, r in ts_df.iterrows():
        ents = conn_to_entities.get(r["Taker Connection ID"], set())
        tsi = str(r["Taker Stream ID"])
        looks_label = ("/" in tsi) or ("-" in tsi) or ("Equities" in tsi) or ("Spot" in tsi) or ("Trade" in tsi)
        if looks_label:
            for ent in ents:
                obs_rows.append((ent, tsi))
        else:
            # Join SD on same opaque stream id
            match_sd = sd_df[sd_df["Taker Stream ID"] == tsi]
            for _, rsd in match_sd.iterrows():
                for ent in ents:
                    obs_rows.append((ent, rsd["Symbol Group Path"]))

    # SD-only rows: entity owns this stream id
    for _, r in sd_df.iterrows():
        tsi_upper = str(r["Taker Stream ID"]).upper()
        for ent, conns in entity_to_conns.items():
            if tsi_upper in conns:
                obs_rows.append((ent, r["Symbol Group Path"]))

    obs_df = pd.DataFrame(obs_rows, columns=["Entity Name", "Observed String"]).drop_duplicates()
    obs_df["Matched Keys"] = obs_df["Observed String"].apply(lambda s: apply_rules_get_keys(s, rules))

    obs_expanded_rows, unclassified, overlaps = [], [], []
    for _, row in obs_df.iterrows():
        keys = sorted(row["Matched Keys"])
        if not keys:
            unclassified.append(row["Observed String"])
        else:
            if len(keys) > 1:
                overlaps.append({"Observed String": row["Observed String"], "Matched Keys": ", ".join(keys)})
            for k in keys:
                obs_expanded_rows.append((row["Entity Name"], k))

    obs_expanded = pd.DataFrame(obs_expanded_rows, columns=["Entity Name", "SF Key"]).drop_duplicates()
    overlaps_df = pd.DataFrame(overlaps)
    unclassified = sorted(set(unclassified))
    return obs_expanded, unclassified, overlaps_df


def load_inputs(sf_report_path: Path, entity_to_taker_path: Path, ts_path: Path, sd_path: Path):
    # Entity_to_Taker
    e2t = load_excel_any_cols(entity_to_taker_path, ["Entity Name", "Connection ID"])
    e2t["Entity Name"] = e2t["Entity Name"].astype(str).str.strip()
    e2t["Connection ID"] = e2t["Connection ID"].astype(str).str.strip().str.upper()
    entity_to_conns = e2t.groupby("Entity Name")["Connection ID"].apply(lambda s: set(s.astype(str))).to_dict()
    conn_to_entities = {}
    for ent, conns in entity_to_conns.items():
        for cid in conns:
            conn_to_entities.setdefault(cid, set()).add(ent)

    # TS
    ts = load_excel_any_cols(ts_path, ["Taker Connection ID", "Taker Stream ID", "Valuation Stream"])
    ts["Valuation Stream"] = ts["Valuation Stream"].apply(truthy)
    ts = ts[ts["Valuation Stream"] == True].copy()
    ts["Taker Connection ID"] = ts["Taker Connection ID"].astype(str).str.strip().str.upper()
    ts["Taker Stream ID"] = ts["Taker Stream ID"].astype(str).str.strip()

    # SD
    sd = load_excel_any_cols(sd_path, ["Symbol Group Path", "Taker Stream ID", "Feed to Taker Streams"])
    sd["Feed to Taker Streams"] = sd["Feed to Taker Streams"].apply(truthy)
    sd = sd[sd["Feed to Taker Streams"] == True].copy()
    sd["Symbol Group Path"] = sd["Symbol Group Path"].astype(str).str.strip()
    sd["Taker Stream ID"] = sd["Taker Stream ID"].astype(str).str.strip()

    # SF report (we only need it later for flags, but validate here)
    wb = pd.read_excel(sf_report_path, sheet_name=None, dtype=object)
    sf_sheet = None
    for _, df in wb.items():
        if any(re.sub(r"[^a-z0-9]+", "", str(c).lower()) == "entityname" for c in df.columns):
            sf_sheet = df.copy()
            break
    if sf_sheet is None:
        raise KeyError("SF report must contain 'Entity Name'")
    sf_sheet.columns = [str(c).strip() for c in sf_sheet.columns]
    return entity_to_conns, conn_to_entities, ts, sd, sf_sheet


def sf_active_from_report(sf_sheet: pd.DataFrame, sf_column_map: dict) -> (pd.DataFrame, list):
    """Return sf_active DataFrame(Entity Name, SF Key) and list of unknown SF columns that were truthy."""
    # find boolean-like columns
    flag_cols = [c for c in sf_sheet.columns if c != "Entity Name" and is_flag_col(sf_sheet[c])]

    mapped_cols, unknown_sf_columns = {}, []
    for c in flag_cols:
        key = sf_column_map.get(c.strip().lower())
        if key:
            mapped_cols[c] = key
        else:
            # If the column has any TRUE-like value, and it's not mapped, surface it.
            if any(truthy(v) for v in sf_sheet[c].dropna().tolist()):
                unknown_sf_columns.append(c)

    rows = []
    for _, r in sf_sheet.iterrows():
        ent = str(r["Entity Name"]).strip()
        if not ent:
            continue
        for c, key in mapped_cols.items():
            if truthy(r.get(c)):
                rows.append((ent, key))
    sf_active = pd.DataFrame(rows, columns=["Entity Name", "SF Key"]).drop_duplicates()
    return sf_active, unknown_sf_columns


def reconcile(sf_active: pd.DataFrame, obs_expanded: pd.DataFrame) -> pd.DataFrame:
    """Return rows of (Entity Name, SF Key, Status)."""
    all_pairs = pd.concat([sf_active, obs_expanded], ignore_index=True).drop_duplicates()

    def status_for(ent, key):
        sf_on = ((sf_active["Entity Name"] == ent) & (sf_active["SF Key"] == key)).any()
        hub_on = ((obs_expanded["Entity Name"] == ent) & (obs_expanded["SF Key"] == key)).any()
        if sf_on and hub_on:
            return "Match"
        if sf_on and not hub_on:
            return "SF Only"
        if not sf_on and hub_on:
            return "Hub Only"
        return None

    out = []
    for _, r in all_pairs.iterrows():
        s = status_for(r["Entity Name"], r["SF Key"])
        if s:
            out.append({"Entity Name": r["Entity Name"], "SF Key": r["SF Key"], "Status": s})
    return pd.DataFrame(out).drop_duplicates()


def write_output(out_path: Path,
                 results_df: pd.DataFrame,
                 key_to_display: dict,
                 unclassified: list,
                 overlaps_df: pd.DataFrame,
                 unknown_sf_columns: list,
                 obs_expanded: pd.DataFrame):
    # attach SF Display for the final Results sheet
    disp = pd.DataFrame([{"SF Key": k, "SF Display": key_to_display.get(k, k)} for k in key_to_display])
    res = results_df.merge(disp, on="SF Key", how="left")
    res["SF Display"] = res["SF Display"].fillna(res["SF Key"])
    res = res[["Entity Name", "SF Display", "Status", "SF Key"]]  # keep SF Key as reference; drop if undesired
    res = res.sort_values(["Entity Name", "SF Display", "Status"]).reset_index(drop=True)

    with pd.ExcelWriter(out_path, engine="xlsxwriter") as writer:
        res[["Entity Name", "SF Display", "Status"]].to_excel(writer, index=False, sheet_name="Results")
        disp.to_excel(writer, index=False, sheet_name="Key->Display")
        pd.DataFrame({"Unclassified Observed Strings": unclassified}).to_excel(writer, index=False, sheet_name="Unclassified Observed")
        if overlaps_df is not None and not overlaps_df.empty:
            overlaps_df.to_excel(writer, index=False, sheet_name="Overlapping Patterns")
        if unknown_sf_columns:
            pd.DataFrame({"Unknown SF Columns (truthy but unmapped)": unknown_sf_columns}).to_excel(writer, index=False, sheet_name="Unknown SF Columns")
        obs_expanded.to_excel(writer, index=False, sheet_name="Observed (Entity x Key)")


# ----------------------- CLI -----------------------

def main():
    ap = argparse.ArgumentParser(description="Reconciliation (SF_Patterns patterns-only)")
    ap.add_argument("--sf_patterns", required=True, type=Path, help="SF_Patterns.xlsx")
    ap.add_argument("--sf_report", required=True, type=Path, help="Market Data SF Report.xlsx")
    ap.add_argument("--entity_to_taker", required=True, type=Path, help="Entity_to_Taker.xlsx")
    ap.add_argument("--ts", required=True, type=Path, help="Taker Stream Consolidated1.xlsx")
    ap.add_argument("--sd", required=True, type=Path, help="Settings Default Consolidated gpt.xlsx")
    ap.add_argument("--output", required=True, type=Path, help="Output .xlsx")
    args = ap.parse_args()

    # Load pattern rules and mappings
    rules, key_to_display, sf_column_map = load_patterns(args.sf_patterns)

    # Load inputs
    entity_to_conns, conn_to_entities, ts_df, sd_df, sf_sheet = load_inputs(
        args.sf_report, args.entity_to_taker, args.ts, args.sd
    )

    # Build observed evidence from TS/SD
    obs_expanded, unclassified, overlaps_df = build_observed(
        entity_to_conns, conn_to_entities, ts_df, sd_df, rules
    )

    # SF actives via explicit SF Column mapping
    sf_active, unknown_sf_columns = sf_active_from_report(sf_sheet, sf_column_map)

    # Reconcile per (Entity, SF Key)
    results_df = reconcile(sf_active, obs_expanded)

    # Write outputs
    write_output(
        args.output,
        results_df,
        key_to_display,
        unclassified,
        overlaps_df,
        unknown_sf_columns,
        obs_expanded,
    )

    print(f"Done. Wrote Results to {args.output}")


if __name__ == "__main__":
    main()
```
